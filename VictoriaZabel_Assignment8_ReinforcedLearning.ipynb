{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOqudPjeUhMPox6yTLlcyfb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victoriaazabel/SEdigitalorganizations/blob/main/VictoriaZabel_Assignment8_ReinforcedLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reinforced Learning\n",
        "- For the final assignment I chose the topic reinforced learning because this is a new concept for me and we have worked with simple language models before.\n",
        "\n",
        "- I chose to work with the alien moon landing at first for a better overview\n",
        "\n",
        "\n",
        "# Goals\n",
        "- Use Gymnasium, the environment library\n",
        "- Use Stable-Baselines3, the deep reinforcement learning library\n",
        "- Understand the code behind each step presented in presentation"
      ],
      "metadata": {
        "id": "3RsbWwvWk-Jg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: install dependencies to create a virtual screen and see what's going on\n",
        "!apt install swig cmake"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qu5alZ6e9Ii8",
        "outputId": "3be339b8-228e-492e-a7f1-a70023a91bc7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "swig is already the newest version (4.0.1-5build1).\n",
            "cmake is already the newest version (3.16.3-1ubuntu1.20.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 16 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: this will install virtual screen libraries and create and run a virtual screen\n",
        "!sudo apt-get update\n",
        "!apt install python-opengl\n",
        "!apt install ffmpeg\n",
        "!apt install xvfb\n",
        "!pip3 install pyvirtualdisplay"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWcBlLZH9WTP",
        "outputId": "9ea01918-5867-422a-d46e-6317177b34ad"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu focal-security InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu focal-updates InRelease\n",
            "Hit:6 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu focal-backports InRelease\n",
            "Hit:8 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "Hit:9 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "Hit:10 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "Hit:11 http://ppa.launchpad.net/ubuntugis/ppa/ubuntu focal InRelease\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python-opengl is already the newest version (3.1.0+dfsg-2build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 16 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.2.7-0ubuntu0.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 16 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:1.20.13-1ubuntu1~20.04.8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 16 not upgraded.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.10/dist-packages (3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# force restart run time for the libaries\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "ltrqM5cw9mpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Virtual display\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_a_Jkv792m2",
        "outputId": "8ecc427a-e91f-4215-a2d6-22e9b39b5717"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7fb0a8b6eec0>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym"
      ],
      "metadata": {
        "id": "1hiAdMXkBcIo"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install stable-baselines3"
      ],
      "metadata": {
        "id": "qxrAzebYBnXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.monitor import Monitor"
      ],
      "metadata": {
        "id": "8xPEZpu_97fF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With Gymnasium:\n",
        "1. Create our environment using gymnasium.make()\n",
        "2. Reset the environment to its initial state with observation = env.reset()\n",
        "\n",
        "\n",
        "At each step:\n",
        "3. Get an action using our model (in our example we take a random action)\n",
        "4. Using env.step(action), we perform this action in the environment and get:\n",
        "\n",
        "\n",
        "- observation: The new state (st+1)\n",
        "- reward: The reward we get after executing the action\n",
        "- terminated: Indicates if the episode terminated (agent reach the terminal state)\n",
        "- truncated: Introduced with this new version, it indicates a timelimit or if an agent go out of bounds of the environment for instance.\n",
        "- info: A dictionary that provides additional information (depends on the environment)"
      ],
      "metadata": {
        "id": "Gtm084RpAN7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# error cuz this was missing but wasn't mentioned, gymnasiym box2d"
      ],
      "metadata": {
        "id": "ivBaBT9UCa9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium[box2d]"
      ],
      "metadata": {
        "id": "e4QTAhXLCSym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create our environment\n",
        "env = gym.make(\"LunarLander-v2\")\n",
        "\n",
        "# reset\n",
        "observation, info = env.reset()\n",
        "\n",
        "# for random actions, step 3\n",
        "for _ in range(20):\n",
        "  action = env.action_space.sample()\n",
        "  print(\"Action taken:\", action)\n",
        "\n",
        "  # perform action in the environment and get\n",
        "  # next_state, reward, terminated, truncated and info\n",
        "  observation, reward, terminated, truncated, info = env.step(action)\n",
        "\n",
        "  # terminated (in our case we land, crashed) or truncated (timeout)\n",
        "  if terminated or truncated:\n",
        "      # Reset the environment\n",
        "      print(\"Environment is reset\")\n",
        "      observation, info = env.reset()\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "id": "ScPpg4zuB5VE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# agent = alien, goal = land correctly on the moon\n",
        "# learn to adapt speed and position / up down at angles\n",
        "\n",
        "#  recreate our environment\n",
        "\n",
        "env = gym.make(\"LunarLander-v2\")\n",
        "env.reset()\n",
        "\n",
        "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
        "print(\"Observation Space Shape\", env.observation_space.shape)\n",
        "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AadDNMmeCt0u",
        "outputId": "bc1dc120-55d3-4582-c3be-fa942bf81fcb"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "_____OBSERVATION SPACE_____ \n",
            "\n",
            "Observation Space Shape (8,)\n",
            "Sample observation [-31.308905  -58.320457    3.2222536  -1.9791884  -1.9628882  -1.0690985\n",
            "   0.5332321   0.4256067]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output: Observation Space Shape (8,) = vector of size 8\n",
        "Each value contains different information about the lander:\n",
        "\n",
        "- Horizontal pad coordinate (x)\n",
        "- Vertical pad coordinate (y)\n",
        "- Horizontal speed (x)\n",
        "- Vertical speed (y)\n",
        "- Angle\n",
        "- Angular speed\n",
        "- If the left leg contact point has touched the land\n",
        "- If the right leg contact point has touched the land\n",
        "\n",
        "--> Observation Space Shape (8,)\n",
        "Sample observation [-31.308905  -58.320457    3.2222536  -1.9791884  -1.9628882  -1.0690985\n",
        "   0.5332321   0.4256067]"
      ],
      "metadata": {
        "id": "VbMkwkyHDNur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# discrete action space: limited to four possibilities\n",
        "# Action 0: Do nothing,\n",
        "# Action 1: Fire left orientation engine,\n",
        "# Action 2: Fire the main engine,\n",
        "# Action 3: Fire right orientation engine\n",
        "\n",
        "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
        "print(\"Action Space Shape\", env.action_space.n)\n",
        "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFS2s4Q8DmCo",
        "outputId": "9af2480c-e547-4219-a970-11357795e49e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " _____ACTION SPACE_____ \n",
            "\n",
            "Action Space Shape 4\n",
            "Action Space Sample 3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reward function:\n",
        "- After every step a reward is granted.\n",
        "- The total reward of an episode is the sum of the rewards for all the steps within that episode.\n",
        "\n",
        "\n",
        "For each step, the reward:\n",
        "- Is increased/decreased the closer/further the lander is to the landing pad.\n",
        "- Is increased/decreased the slower/faster the lander is moving.\n",
        "- Is decreased the more the lander is tilted (angle not horizontal).\n",
        "- Is increased by 10 points for each leg that is in contact with the ground.\n",
        "- Is decreased by 0.03 points each frame a side engine is firing.\n",
        "- Is decreased by 0.3 points each frame the main engine is firing.\n",
        "\n",
        "The episode receive an additional reward of -100 or +100 points for crashing or landing safely respectively.\n",
        "\n",
        "\n",
        "An episode is considered a solution if it scores at least 200 points"
      ],
      "metadata": {
        "id": "Mx9fONhrECin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/DLR-RM/stable-baselines3"
      ],
      "metadata": {
        "id": "IulqnpDQE2Vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from now on we'll need these dumb impossible to import packages\n",
        "\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_util import make_vec_env\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.monitor import Monitor"
      ],
      "metadata": {
        "id": "iKsoo7SrExJH"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vectorized environment created to  stack multiple independent environments into a single environment, mix of 16 environments\n",
        "# why? --> more diverse experiences during training\n",
        "env = make_vec_env('LunarLander-v2', n_envs=16)"
      ],
      "metadata": {
        "id": "psDNogg9EZwD"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use deep RL library to create model\n",
        "# already created environemtn so now we need the model\n",
        "\n",
        "# We added some parameters to accelerate the training\n",
        "model = PPO(\n",
        "    policy = 'MlpPolicy', # which method\n",
        "    env = env, # use previous environment\n",
        "    n_steps = 1024, # interactions  agent will have with the environment before collecting experience to update its policy\n",
        "    batch_size = 64, # samples that will be used in each training iteration\n",
        "    n_epochs = 4, # run throughs\n",
        "    gamma = 0.999, # discount rate, higher so smaller discount = our agent cares more about the long-term reward\n",
        "    gae_lambda = 0.98, # ???? how much the agent values the current estimate of advantages compared to future estimates?????????\n",
        "    ent_coef = 0.01, # exploration / exploitation trade off = exploiting here\n",
        "    verbose=1) # 1 enables the algorithm to print progress information and updates while training."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QtCP7b8hFXsF",
        "outputId": "be92d82c-6356-499b-eec2-b625129824fa"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training model\n",
        "\n",
        "# 1,000,000 timesteps for improvement of results\n",
        "\n",
        "model.learn(total_timesteps=1000000)\n",
        "\n",
        "model_name = \"ppo-LunarLander-v2\"\n",
        "model.save(model_name)"
      ],
      "metadata": {
        "id": "8d1CgBClIAaT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluating agent, not model!\n",
        "# create an evaluation environment\n",
        "# Stable-Baselines3 method: evaluate_policy according to rewards collected in evalutaion cuz the goal is to have the highest points / rewards\n",
        "\n",
        "eval_env = Monitor(gym.make(\"LunarLander-v2\"))\n",
        "\n",
        "# using prev model, new environment\n",
        "# evaluation episodes = 10, policy will be evaluated for 10 episodes\n",
        "# determ = True, the policy will select the actions with the highest probability / = False, value based/ Acts on biggest value to attain goal\n",
        "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
        "\n",
        "# mean to two dp, sd range\n",
        "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJ8oIUUNITB6",
        "outputId": "96b180c4-ae4e-4e5c-a75d-7aa931c41744"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mean_reward=269.36 +/- 15.96156252416331\n"
          ]
        }
      ]
    }
  ]
}