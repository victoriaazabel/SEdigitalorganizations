{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/victoriaazabel/SEdigitalorganizations/blob/main/VictoriaZabel_Assignment%20and%20Learning%20Portfolio4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Learning Portfolio: Theory\n",
        "\n",
        "What is \"torch.cat()\" and \".view(-1, 28*28)\" doing in the beginning of the \"The MNIST Loss Function\" chapter?\n",
        "\n",
        "Can you draw the neuronal network, which is manually trained in chapter \"The MNIST Loss Function\"?\n",
        "\n",
        "Why is it not possible to use the accuracy as loss function?\n",
        "\n",
        "What is the defined `mnist_loss` function doing?\n",
        "\n",
        "\n",
        "```\n",
        "def mnist_loss(predictions, targets):\n",
        "    return torch.where(targets==1, 1-predictions, predictions).mean()\n",
        "```\n",
        "\n",
        "Why do we need additionaly the sigmoid() function? What is it technically in our TLU?\n",
        "\n",
        "Again, what are mini batches, why are we using them and why should they be shuffeld?"
      ],
      "metadata": {
        "id": "iIBgQ5f43H6z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Answers\n",
        "1. In the beginning of the \"MNIST Loss Function\" chapter, the \"torch.cat()\" and \".view(-1, 28*28)\" phrases are part of the train_x function. torch.cat is responsible for linking a sequence of tensors along an existing dimension, hence not changing the dimension of the tensors. The view parameters changes this from a list of matrices to one of vectors (from a rank-3 to rank-2 tensor). Additionally, the -1 allows us to view the axis as large as necessary for our data. The 28 x 28 is the number of pixels for the image.\n",
        "\n",
        "2. check last code section after linear_model = nn.Linear(28*28,1)() has been defined.\n",
        "\n",
        "3. It is not possible to use the accuracy as the loss function because loss functions need to be differentiable to minimize the difference of predicted and actual outputs, something that the accuracy function is not capable of doing.\n",
        "\n",
        "4. The mnist_loss function measures the difference between target values and predictions, meaning that it measures the distance. For example, how distant each prediction is from 1 if it should be 1, and how distant it is from 0 if it should be 0. Following this, it will take the mean of the distances.\n",
        "\n",
        "5. The sigmoid() function is needed in addition to our mnist_lost function because the mnist_loss function assumes that predictions remain between 0 and 1. To ensure and double check that this is the case we need to use the sigmoid() function. In our TLU, it will have either the number 0 or 1 as its ouput.\n",
        "\n",
        "\n",
        "6. Mini Batches are used to calculate the average loss for a few data items at a time to obtain estimates of the dataset's gradients from the loss function. The larger the batch size, the more accurate and stable the estimate will be. They should be randomly shuffled on every epoch to improve generalization. This needs to be done through varying during training and deciding which data items to put in mini batches."
      ],
      "metadata": {
        "id": "UNThOS-W8V2X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Practical Part\n",
        "\n",
        "Try to understand all parts of the code needed to manually train a single TLU/Perceptron, so use and copy all parts of the code from \"First Try: Pixel Similarity\" to the \"Putting it all together\" chapter. In the second step, use an optimizer, a second layer, and a ReLU as a hidden activation function to train a simple neural network. When copying the code, think carefully about what you really need and how you can summarize it as compactly as possible. (Probably each attempt requires about 15 lines of code.)"
      ],
      "metadata": {
        "id": "aoQq7z5D3XXV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxwyNuzj3DYu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "17eca9ac-b43b-4f93-b16e-1e3f84f989cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m719.8/719.8 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hMounted at /content/gdrive\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='3219456' class='' max='3214948' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      100.14% [3219456/3214948 00:00&lt;00:00]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#YOUR TASK: Manually train a single layer perceptron without using an optimizer.\n",
        "\n",
        "#hide\n",
        "! [ -e /content ] && pip install -Uqq fastbook\n",
        "import fastbook\n",
        "fastbook.setup_book()\n",
        "from fastai.vision.all import *\n",
        "from fastbook import *\n",
        "\n",
        "matplotlib.rc('image', cmap='Greys')\n",
        "\n",
        "path = untar_data(URLs.MNIST_SAMPLE)\n",
        "\n",
        "#hide\n",
        "Path.BASE_PATH = path\n",
        "\n",
        "threes = (path/'train'/'3').ls().sorted()\n",
        "sevens = (path/'train'/'7').ls().sorted()\n",
        "\n",
        "#first try:\n",
        "seven_tensors = [tensor(Image.open(o)) for o in sevens]\n",
        "three_tensors = [tensor(Image.open(o)) for o in threes]\n",
        "\n",
        "stacked_sevens = torch.stack(seven_tensors).float()/255\n",
        "stacked_threes = torch.stack(three_tensors).float()/255\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#computing...:\n",
        "\n",
        "valid_3_tens = torch.stack([tensor(Image.open(o))\n",
        "                            for o in (path/'valid'/'3').ls()])\n",
        "valid_3_tens = valid_3_tens.float()/255\n",
        "valid_7_tens = torch.stack([tensor(Image.open(o))\n",
        "                            for o in (path/'valid'/'7').ls()])\n",
        "valid_7_tens = valid_7_tens.float()/255"
      ],
      "metadata": {
        "id": "45q5uGTO_lI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loss\n",
        "\n",
        "train_x = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28*28)\n",
        "train_y = tensor([1]*len(threes) + [0]*len(sevens)).unsqueeze(1)\n",
        "dset = list(zip(train_x,train_y))\n",
        "\n",
        "valid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-1, 28*28)\n",
        "valid_y = tensor([1]*len(valid_3_tens) + [0]*len(valid_7_tens)).unsqueeze(1)\n",
        "valid_dset = list(zip(valid_x,valid_y))\n",
        "\n",
        "def init_params(size, std=1.0):\n",
        "  return (torch.randn(size)*std).requires_grad_()\n",
        "\n",
        "weights = init_params((28*28,1))\n",
        "bias = init_params(1)\n",
        "\n",
        "def linear1(xb):\n",
        "  return xb@weights + bias\n",
        "\n",
        "def mnist_loss(predictions, targets):\n",
        "    predictions = predictions.sigmoid()\n",
        "    return torch.where(targets==1, 1-predictions, predictions).mean()"
      ],
      "metadata": {
        "id": "y-EK25-ZCPfE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# putting...\n",
        "\n",
        "dl = DataLoader(dset, batch_size=256)\n",
        "xb,yb = first(dl)\n",
        "xb.shape,yb.shape\n",
        "\n",
        "valid_dl = DataLoader(valid_dset, batch_size=256)\n",
        "\n",
        "def calc_grad(xb, yb, model):\n",
        "    preds = model(xb)\n",
        "    loss = mnist_loss(preds, yb)\n",
        "    loss.backward()\n",
        "\n",
        "# second time\n",
        "\n",
        "def train_epoch(model, lr, params):\n",
        "    for xb,yb in dl:\n",
        "        calc_grad(xb, yb, model)\n",
        "        for p in params:\n",
        "            p.data -= p.grad*lr\n",
        "            p.grad.zero_()\n",
        "\n",
        "def batch_accuracy(xb, yb):\n",
        "    preds = xb.sigmoid()\n",
        "    correct = (preds>0.5) == yb\n",
        "    return correct.float().mean()\n",
        "\n",
        "def validate_epoch(model):\n",
        "    accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl]\n",
        "    return round(torch.stack(accs).mean().item(), 4)\n",
        "\n",
        "lr = 1.\n",
        "params = weights,bias\n",
        "train_epoch(linear1, lr, params)\n",
        "validate_epoch(linear1)\n",
        "\n",
        "for i in range(20):\n",
        "    train_epoch(linear1, lr, params)\n",
        "    print(validate_epoch(linear1), end=' ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Naz1IGdgC-Fq",
        "outputId": "6679b76e-15f4-43aa-961b-14bf30683d48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8505 0.9008 0.9291 0.9389 0.9443 0.9526 0.9545 0.9589 0.9613 0.9623 0.9623 0.9633 0.9638 0.9647 0.9662 0.9667 0.9672 0.9672 0.9677 0.9682 "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#YOUR TASK: Train a simple two-layer neural network (two perceptrons + hidden activation function) with built-in functions and an optimizer."
      ],
      "metadata": {
        "id": "UGsLRFtMbyRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "linear_model = nn.Linear(28*28,1)\n",
        "\n",
        "w,b = linear_model.parameters()\n",
        "\n",
        "dls = DataLoaders(dl, valid_dl)\n",
        "\n",
        "learn = Learner(dls, nn.Linear(28*28,1), opt_func=SGD,\n",
        "                loss_func=mnist_loss, metrics=batch_accuracy)\n",
        "\n",
        "simple_net = nn.Sequential(\n",
        "    nn.Linear(28*28,30),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(30,1)\n",
        ")\n",
        "\n",
        "learn = Learner(dls, simple_net, opt_func=SGD,\n",
        "                loss_func=mnist_loss, metrics=batch_accuracy)\n",
        "\n",
        "#hide_output\n",
        "learn.fit(40, 0.1)\n",
        "\n",
        "learn.recorder.values[-1][2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lPvcx4FmI_fW",
        "outputId": "95d7a705-f4c4-416f-bf4b-818e50067c07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>batch_accuracy</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.326422</td>\n",
              "      <td>0.425299</td>\n",
              "      <td>0.504416</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.155275</td>\n",
              "      <td>0.235706</td>\n",
              "      <td>0.801276</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.085114</td>\n",
              "      <td>0.117112</td>\n",
              "      <td>0.913150</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.055286</td>\n",
              "      <td>0.078646</td>\n",
              "      <td>0.940137</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.041516</td>\n",
              "      <td>0.061311</td>\n",
              "      <td>0.954858</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.034534</td>\n",
              "      <td>0.051605</td>\n",
              "      <td>0.963690</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.030557</td>\n",
              "      <td>0.045485</td>\n",
              "      <td>0.966143</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.027993</td>\n",
              "      <td>0.041304</td>\n",
              "      <td>0.966143</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.026156</td>\n",
              "      <td>0.038262</td>\n",
              "      <td>0.968106</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.024739</td>\n",
              "      <td>0.035939</td>\n",
              "      <td>0.968597</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.023595</td>\n",
              "      <td>0.034096</td>\n",
              "      <td>0.970559</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.022644</td>\n",
              "      <td>0.032589</td>\n",
              "      <td>0.972031</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.021835</td>\n",
              "      <td>0.031323</td>\n",
              "      <td>0.973503</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.021136</td>\n",
              "      <td>0.030235</td>\n",
              "      <td>0.974485</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.020525</td>\n",
              "      <td>0.029288</td>\n",
              "      <td>0.974975</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.019984</td>\n",
              "      <td>0.028451</td>\n",
              "      <td>0.975466</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.019501</td>\n",
              "      <td>0.027706</td>\n",
              "      <td>0.976448</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.019065</td>\n",
              "      <td>0.027036</td>\n",
              "      <td>0.976938</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.018670</td>\n",
              "      <td>0.026432</td>\n",
              "      <td>0.978410</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.018307</td>\n",
              "      <td>0.025884</td>\n",
              "      <td>0.978410</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.017974</td>\n",
              "      <td>0.025385</td>\n",
              "      <td>0.979392</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.017665</td>\n",
              "      <td>0.024930</td>\n",
              "      <td>0.979392</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.017378</td>\n",
              "      <td>0.024511</td>\n",
              "      <td>0.979392</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.017109</td>\n",
              "      <td>0.024127</td>\n",
              "      <td>0.980373</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.016858</td>\n",
              "      <td>0.023771</td>\n",
              "      <td>0.980373</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.016622</td>\n",
              "      <td>0.023443</td>\n",
              "      <td>0.980373</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.016399</td>\n",
              "      <td>0.023139</td>\n",
              "      <td>0.980864</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.016188</td>\n",
              "      <td>0.022856</td>\n",
              "      <td>0.980864</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.015989</td>\n",
              "      <td>0.022592</td>\n",
              "      <td>0.980864</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.015799</td>\n",
              "      <td>0.022346</td>\n",
              "      <td>0.981354</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.015619</td>\n",
              "      <td>0.022116</td>\n",
              "      <td>0.981354</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.015447</td>\n",
              "      <td>0.021901</td>\n",
              "      <td>0.981845</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.015283</td>\n",
              "      <td>0.021699</td>\n",
              "      <td>0.981845</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.015126</td>\n",
              "      <td>0.021509</td>\n",
              "      <td>0.982336</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.014977</td>\n",
              "      <td>0.021330</td>\n",
              "      <td>0.982336</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.014833</td>\n",
              "      <td>0.021161</td>\n",
              "      <td>0.982336</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.014695</td>\n",
              "      <td>0.021002</td>\n",
              "      <td>0.982826</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.014562</td>\n",
              "      <td>0.020850</td>\n",
              "      <td>0.982826</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.014435</td>\n",
              "      <td>0.020707</td>\n",
              "      <td>0.983317</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.014312</td>\n",
              "      <td>0.020570</td>\n",
              "      <td>0.983317</td>\n",
              "      <td>00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.983316957950592"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Code, source: appsilon (2022)\n",
        "\n",
        "!pip install torchviz\n",
        "\n",
        "from torchviz import make_dot\n",
        "\n",
        "model = nn.Linear(28*28,1)\n",
        "y = model(valid_x)\n",
        "\n",
        "make_dot(y.mean(), params=dict(model.named_parameters()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 707
        },
        "id": "qmnylukYM91M",
        "outputId": "0b8cea9b-4503-45e7-fe4d-90697081e994"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: torchviz in /usr/local/lib/python3.10/dist-packages (0.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from torchviz) (2.0.0+cu118)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.10/dist-packages (from torchviz) (0.20.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->torchviz) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchviz) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->torchviz) (16.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->torchviz) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->torchviz) (1.3.0)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: %3 Pages: 1 -->\n<svg width=\"216pt\" height=\"336pt\"\n viewBox=\"0.00 0.00 216.00 336.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 332)\">\n<title>%3</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-332 212,-332 212,4 -4,4\"/>\n<!-- 139937347520688 -->\n<g id=\"node1\" class=\"node\">\n<title>139937347520688</title>\n<polygon fill=\"#caff70\" stroke=\"black\" points=\"130.5,-31 76.5,-31 76.5,0 130.5,0 130.5,-31\"/>\n<text text-anchor=\"middle\" x=\"103.5\" y=\"-7\" font-family=\"monospace\" font-size=\"10.00\"> ()</text>\n</g>\n<!-- 139934393568128 -->\n<g id=\"node2\" class=\"node\">\n<title>139934393568128</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"151,-86 56,-86 56,-67 151,-67 151,-86\"/>\n<text text-anchor=\"middle\" x=\"103.5\" y=\"-74\" font-family=\"monospace\" font-size=\"10.00\">MeanBackward0</text>\n</g>\n<!-- 139934393568128&#45;&gt;139937347520688 -->\n<g id=\"edge7\" class=\"edge\">\n<title>139934393568128&#45;&gt;139937347520688</title>\n<path fill=\"none\" stroke=\"black\" d=\"M103.5,-66.79C103.5,-60.07 103.5,-50.4 103.5,-41.34\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"107,-41.19 103.5,-31.19 100,-41.19 107,-41.19\"/>\n</g>\n<!-- 139934393565872 -->\n<g id=\"node3\" class=\"node\">\n<title>139934393565872</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"154,-141 53,-141 53,-122 154,-122 154,-141\"/>\n<text text-anchor=\"middle\" x=\"103.5\" y=\"-129\" font-family=\"monospace\" font-size=\"10.00\">AddmmBackward0</text>\n</g>\n<!-- 139934393565872&#45;&gt;139934393568128 -->\n<g id=\"edge1\" class=\"edge\">\n<title>139934393565872&#45;&gt;139934393568128</title>\n<path fill=\"none\" stroke=\"black\" d=\"M103.5,-121.75C103.5,-114.8 103.5,-104.85 103.5,-96.13\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"107,-96.09 103.5,-86.09 100,-96.09 107,-96.09\"/>\n</g>\n<!-- 139934393578304 -->\n<g id=\"node4\" class=\"node\">\n<title>139934393578304</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"101,-196 0,-196 0,-177 101,-177 101,-196\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 139934393578304&#45;&gt;139934393565872 -->\n<g id=\"edge2\" class=\"edge\">\n<title>139934393578304&#45;&gt;139934393565872</title>\n<path fill=\"none\" stroke=\"black\" d=\"M59.25,-176.75C66.97,-169.03 78.4,-157.6 87.72,-148.28\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"90.31,-150.64 94.91,-141.09 85.36,-145.69 90.31,-150.64\"/>\n</g>\n<!-- 139937347525728 -->\n<g id=\"node5\" class=\"node\">\n<title>139937347525728</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"77.5,-262 23.5,-262 23.5,-232 77.5,-232 77.5,-262\"/>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-250\" font-family=\"monospace\" font-size=\"10.00\">bias</text>\n<text text-anchor=\"middle\" x=\"50.5\" y=\"-239\" font-family=\"monospace\" font-size=\"10.00\"> (1)</text>\n</g>\n<!-- 139937347525728&#45;&gt;139934393578304 -->\n<g id=\"edge3\" class=\"edge\">\n<title>139937347525728&#45;&gt;139934393578304</title>\n<path fill=\"none\" stroke=\"black\" d=\"M50.5,-231.84C50.5,-224.21 50.5,-214.7 50.5,-206.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"54,-206.27 50.5,-196.27 47,-206.27 54,-206.27\"/>\n</g>\n<!-- 139934393571152 -->\n<g id=\"node6\" class=\"node\">\n<title>139934393571152</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"196,-196 119,-196 119,-177 196,-177 196,-196\"/>\n<text text-anchor=\"middle\" x=\"157.5\" y=\"-184\" font-family=\"monospace\" font-size=\"10.00\">TBackward0</text>\n</g>\n<!-- 139934393571152&#45;&gt;139934393565872 -->\n<g id=\"edge4\" class=\"edge\">\n<title>139934393571152&#45;&gt;139934393565872</title>\n<path fill=\"none\" stroke=\"black\" d=\"M148.58,-176.75C140.72,-169.03 129.07,-157.6 119.58,-148.28\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"121.84,-145.6 112.25,-141.09 116.94,-150.59 121.84,-145.6\"/>\n</g>\n<!-- 139934393574464 -->\n<g id=\"node7\" class=\"node\">\n<title>139934393574464</title>\n<polygon fill=\"lightgrey\" stroke=\"black\" points=\"208,-256.5 107,-256.5 107,-237.5 208,-237.5 208,-256.5\"/>\n<text text-anchor=\"middle\" x=\"157.5\" y=\"-244.5\" font-family=\"monospace\" font-size=\"10.00\">AccumulateGrad</text>\n</g>\n<!-- 139934393574464&#45;&gt;139934393571152 -->\n<g id=\"edge5\" class=\"edge\">\n<title>139934393574464&#45;&gt;139934393571152</title>\n<path fill=\"none\" stroke=\"black\" d=\"M157.5,-237.37C157.5,-229.25 157.5,-216.81 157.5,-206.39\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"161,-206.17 157.5,-196.17 154,-206.17 161,-206.17\"/>\n</g>\n<!-- 139937347533568 -->\n<g id=\"node8\" class=\"node\">\n<title>139937347533568</title>\n<polygon fill=\"lightblue\" stroke=\"black\" points=\"193,-328 122,-328 122,-298 193,-298 193,-328\"/>\n<text text-anchor=\"middle\" x=\"157.5\" y=\"-316\" font-family=\"monospace\" font-size=\"10.00\">weight</text>\n<text text-anchor=\"middle\" x=\"157.5\" y=\"-305\" font-family=\"monospace\" font-size=\"10.00\"> (1, 784)</text>\n</g>\n<!-- 139937347533568&#45;&gt;139934393574464 -->\n<g id=\"edge6\" class=\"edge\">\n<title>139937347533568&#45;&gt;139934393574464</title>\n<path fill=\"none\" stroke=\"black\" d=\"M157.5,-297.8C157.5,-288.7 157.5,-276.79 157.5,-266.9\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"161,-266.84 157.5,-256.84 154,-266.84 161,-266.84\"/>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.graphs.Digraph at 0x7f4503d23eb0>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ]
}